# Transformer 기반 AmodalFusionNet의 상세 로직 분석

## 가변적인 completion 개수 처리 방법

Transformer 기반 AmodalFusionNet은 각 이미지마다 다른 수의 객체와 completion 결과를 처리하기 위해 다음과 같은 전략을 사용합니다:

### 1. 최대 completion 수 정의 (`n_max_completions`)

모델은 미리 정의된 최대 completion 수(`n_max_completions`, 기본값 5)를 사용합니다. 이 값은 모델 입력 채널 수를 고정하기 위한 것입니다.

```python
# 최대 completion 수 정의 (아래는 모델 초기화 시)
n_max_completions = 5
in_channels = 3 + 1 + 3 * n_max_completions  # 부분 이미지(3) + 마스크(1) + 완성 이미지들(3*N)
```

### 2. 패딩 전략

각 이미지에서 실제 completion 수가 `n_max_completions`보다 적을 경우, 나머지 슬롯을 "0으로 채워진" 검은색 이미지로 채웁니다:

```python
# 데이터셋 클래스의 __getitem__ 메서드에서
completions = []
for j in range(1, self.n_max_completions + 1):
    comp_path = item_path / f"{image_id}_comp{j}.png"
    if comp_path.exists():
        comp = Image.open(comp_path).convert("RGB")
    else:
        # n_max_completions보다 적은 경우 검은색 이미지로 채움
        comp = Image.new("RGB", X.size, color=(0, 0, 0))
    completions.append(comp)
```

이렇게 함으로써 모든 샘플이 항상 고정된 수의 completion을 가지게 됩니다. 실제 completion이 없는 슬롯은 검은색 이미지로 채워집니다.

### 3. 가중치 학습을 통한 무시 메커니즘

모델은 훈련 과정에서 존재하지 않는(패딩된) completion에 낮은 가중치를 할당하도록 학습합니다. 모델의 출력은 소프트맥스 함수를 통과하기 때문에, 실제 객체가 없는 패딩된 completion에는 거의 0에 가까운 가중치가 할당되게 됩니다:

```python
# 모델의 forward 메서드 마지막 부분
weight_maps = self.output_head(x)  # [B, n_completions, H, W]
return F.softmax(weight_maps, dim=1)  # 소프트맥스로 각 픽셀별 가중치 정규화
```

훈련 도중 L1, LPIPS 손실이 적용되면서 실제 객체가 있는 completion만 선택하도록 가중치가 조정됩니다.

## 모델 설계 로직 상세 분석

### 1. Transformer 기반 아키텍처 선택 이유

U-Net 대신 Transformer 아키텍처를 선택한 핵심 이유:

1. **전역 문맥 이해**: Transformer의 self-attention 메커니즘은 이미지의 모든 픽셀 간 관계를 모델링할 수 있습니다. 이는 중첩된 객체들이 있을 때 어떤 객체가 앞에 있고 어떤 객체가 뒤에 있는지와 같은 전역적 관계를 이해하는 데 중요합니다.

2. **객체 간 관계 학습**: Transformer는 이미지 내 서로 다른 영역 간의 관계를 명시적으로 모델링할 수 있습니다. 이는 여러 객체 간의 복잡한 중첩 관계를 파악하는 데 필수적입니다.

3. **적응형 가중치 할당**: Transformer의 attention 메커니즘은 각 픽셀에 대해 가장 관련 있는 문맥에 동적으로 집중할 수 있어 복잡한 장면에서도 정확한 중첩 순서 파악이 가능합니다.

### 2. 전체 아키텍처 흐름

AmodalFusionTransformer의 전체 파이프라인은 다음과 같습니다:

1. **입력 준비**: 
   - 부분 이미지(3채널), 마스크(1채널), N개의 completion 이미지(각 3채널)를 하나의 텐서로 연결
   - 총 입력 채널 수 = 3 + 1 + 3*N

2. **초기 특징 추출 (컨볼루션 인코더)**:
   - 스트라이드 컨볼루션을 통해 이미지 해상도 1/4로 다운샘플링
   - 채널 수를 d_model로 변환

3. **위치 인코딩 추가**:
   - 2D 위치 정보를 특징 맵에 주입하여 공간 정보 보존

4. **Transformer 인코더 처리**:
   - 특징 맵을 1D 시퀀스로 변환 후 self-attention 적용
   - 각 위치는 전체 이미지 문맥 정보 포착

5. **Transformer 디코더 처리**:
   - 학습된 쿼리 임베딩으로 전체 이미지 정보 쿼리
   - 쿼리 결과와 인코더 출력 결합

6. **업샘플링 디코더**:
   - 특징 맵을 원래 해상도로 복원

7. **가중치 맵 생성**:
   - 각 completion에 대한 픽셀별 가중치 맵 생성
   - 소프트맥스로 정규화하여 각 픽셀의 가중치 합이 1이 되도록 함

8. **가중치 기반 융합**:
   - 픽셀별 가중치를 사용하여 N개의 completion 이미지 융합
   - 마스크 영역에만 적용하여 최종 이미지 생성

### 3. 핵심 컴포넌트 상세 분석

#### 3.1 2D 위치 인코딩

Transformer는 원래 순서 정보가 없는 모델이므로, 이미지의 공간 정보를 보존하기 위해 위치 인코딩이 필요합니다:

```python
class PositionalEncoding2D(nn.Module):
    def __init__(self, d_model, max_h=256, max_w=256):
        super().__init__()
        self.d_model = d_model
        
        # 사인 및 코사인 위치 인코딩 계산
        pe = torch.zeros(d_model, max_h, max_w)
        div_term = torch.exp(torch.arange(0, d_model//2, 2) * -(math.log(10000.0) / (d_model//2)))
        
        pos_h = torch.arange(0, max_h).unsqueeze(1).unsqueeze(1).float()
        pos_w = torch.arange(0, max_w).unsqueeze(0).unsqueeze(1).float()
        
        # 높이와 너비에 대한 인코딩 처리
        pe[0:d_model//2:2, :, :] = torch.sin(pos_h * div_term).transpose(1, 2)
        pe[1:d_model//2:2, :, :] = torch.cos(pos_h * div_term).transpose(1, 2)
        pe[d_model//2::2, :, :] = torch.sin(pos_w * div_term)
        pe[d_model//2+1::2, :, :] = torch.cos(pos_w * div_term)
```

이 코드는 각 공간 위치(h, w)에 대해 고유한 위치 인코딩을 생성합니다. 차원의 절반은 높이 정보를, 나머지 절반은 너비 정보를 인코딩합니다. `sin`과 `cos` 함수를 사용해 다른 주파수의 위치 정보를 임베딩합니다. 이를 특징 맵에 더함으로써 모델이 공간 위치를 인식할 수 있게 됩니다.

#### 3.2 컨볼루션 인코더

컨볼루션 인코더는 초기 특징 추출과 다운샘플링을 담당합니다:

```python
self.conv_encoder = nn.Sequential(
    nn.Conv2d(in_channels, 64, kernel_size=7, padding=3, stride=2),  # 1/2 다운샘플링
    nn.BatchNorm2d(64),
    nn.ReLU(inplace=True),
    nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=2),  # 추가 1/2 다운샘플링
    nn.BatchNorm2d(128),
    nn.ReLU(inplace=True),
    nn.Conv2d(128, d_model, kernel_size=1)  # d_model 채널로 조정
)
```

이 모듈은:
- 큰 커널(7x7)로 초기 특징 추출
- 두 번의 스트라이드 컨볼루션으로 해상도 1/4로 감소
- 1x1 컨볼루션으로 채널 수를 `d_model`(기본값 256)로 조정

다운샘플링은 계산 효율성을 위해 필수적입니다. 256x256 이미지의 경우, 다운샘플링 후 64x64 특징 맵이 됩니다. 이는 Transformer에서 처리할 시퀀스 길이가 4,096(64x64)임을 의미합니다.

#### 3.3 Transformer 인코더

인코더는 이미지 전역에 걸친 문맥 정보를 포착합니다:

```python
encoder_layer = nn.TransformerEncoderLayer(
    d_model=d_model, 
    nhead=nhead, 
    dim_feedforward=dim_feedforward, 
    dropout=dropout,
    batch_first=True
)
self.transformer_encoder = nn.TransformerEncoder(
    encoder_layer, 
    num_layers=num_encoder_layers
)
```

이 코드는 `num_encoder_layers`개의 인코더 레이어를 쌓습니다. 각 레이어는:
- `nhead`개의 헤드를 가진 멀티헤드 셀프 어텐션
- 피드포워드 네트워크 (`dim_feedforward` 히든 유닛)
- 드롭아웃 정규화

인코더에 입력하기 전에 2D 특징 맵을 1D 시퀀스로 변환합니다:
```python
x_flat = x.flatten(2).permute(0, 2, 1)  # [B, d_model, H, W] → [B, H*W, d_model]
memory = self.transformer_encoder(x_flat)
```

인코더의 출력 `memory`는 각 위치가 전체 이미지의 문맥을 인식하는 특징 시퀀스입니다.

#### 3.4 Transformer 디코더와 쿼리 임베딩

디코더는 이미지의 전역 정보를 처리합니다:

```python
self.query_embed = nn.Parameter(torch.randn(1, d_model))

decoder_layer = nn.TransformerDecoderLayer(
    d_model=d_model, 
    nhead=nhead, 
    dim_feedforward=dim_feedforward, 
    dropout=dropout,
    batch_first=True
)
self.transformer_decoder = nn.TransformerDecoder(
    decoder_layer, 
    num_layers=num_decoder_layers
)
```

특이한 점은 단일 쿼리 임베딩을 사용한다는 것입니다. 이 임베딩은 배치의 각 이미지에 복제됩니다:
```python
query = self.query_embed.expand(b, -1, -1)  # [1, d_model] → [B, 1, d_model]
decoder_output = self.transformer_decoder(query, memory)  # [B, 1, d_model]
```

이 단일 쿼리는 "전체 이미지의 상태가 어떤가?"라고 물어보는 것과 같습니다. 디코더는 이 쿼리와 인코더 출력(`memory`)의 크로스 어텐션을 통해 전역 정보를 추출합니다.

디코더 출력을 인코더 출력과 결합하여 최종 특징 맵을 생성합니다:
```python
combined = torch.cat([decoder_output, memory], dim=1)
features = combined[:, 1:, :].permute(0, 2, 1).view(b, self.d_model, h_down, w_down)
```

여기서 쿼리 토큰(`decoder_output`)은 제외하고 나머지 토큰들(`memory`)만 사용하여 원래 2D 형태로 복원합니다.

#### 3.5 업샘플링 및 가중치 생성

특징 맵을 원래 해상도로 업샘플링하고 가중치 맵을 생성합니다:

```python
x = F.relu(self.bn1(self.upconv1(features)))  # [B, 128, H/2, W/2]
x = F.relu(self.bn2(self.upconv2(x)))         # [B, 64, H, W]
weight_maps = self.output_head(x)              # [B, n_completions, H, W]
return F.softmax(weight_maps, dim=1)
```

이 단계는:
- 전치 컨볼루션(`upconv`)을 사용해 해상도를 원래 크기로 복원
- 1x1 컨볼루션을 사용해 각 completion에 대한 가중치 맵 생성
- 소프트맥스로 각 픽셀에서 모든 completion의 가중치 합이 1이 되도록 정규화

### 4. 손실 함수 및 훈련 로직

훈련 시 두 가지 주요 손실 함수를 사용합니다:

1. **L1 손실**: 생성된 이미지와 원본 이미지 간의 픽셀별 절대 차이
   ```python
   l1_loss = F.l1_loss(X_input * mask_expanded, X * mask_expanded)
   ```

2. **LPIPS 손실**: 지각적 유사성 측정을 위한 학습된 특징 기반 손실
   ```python
   percept_loss = lpips_loss(X_input * mask_expanded, X * mask_expanded).mean()
   ```

두 손실을 합쳐 최종 손실을 계산합니다:
```python
loss = l1_loss + args.lpips_weight * percept_loss
```

모델은 이 손실을 최소화하도록 훈련됩니다. 특히 중요한 점은 **마스크 영역에서만 손실을 계산**한다는 것입니다(`mask_expanded`로 곱함). 이는 모델이 마스크 영역에서 어떤 completion이 가장 적합한지 학습하도록 합니다.

### 5. 분산 훈련 로직

6개의 GPU를 효율적으로 활용하기 위해 분산 데이터 병렬(DDP) 훈련을 구현합니다:

```python
model = AmodalFusionTransformer(...).to(device)
model = torch.nn.parallel.DistributedDataParallel(
    model, 
    device_ids=[local_rank],
    output_device=local_rank,
    find_unused_parameters=True
)
```

각 GPU는 동일한 모델 사본을 가지며 서로 다른 배치 데이터를 처리합니다. 각 반복 후 그래디언트가 모든 GPU 간에 동기화되고 모델 가중치가 업데이트됩니다.

중요한 부분은 손실과 메트릭 동기화입니다:
```python
train_loss_tensor = torch.tensor([train_loss, train_l1_loss, train_lpips_loss], device=device)
torch.distributed.all_reduce(train_loss_tensor, op=torch.distributed.ReduceOp.SUM)
train_loss_tensor /= (world_size * len(train_loader))
```

`all_reduce` 연산은 모든 GPU의 손실 값을 수집하고 합산하여 각 GPU가 동일한 값을 가지도록 합니다. 이후 GPU 수와 데이터 로더 길이로 나누어 평균을 계산합니다.

### 6. 추론 로직

추론 시, 모델은 입력으로 부분 이미지, 마스크, N개의 completion을 받아 가중치 맵을 생성합니다:

```python
with torch.no_grad():
    weight_maps = model(inputs)

weight_maps_expanded = weight_maps.unsqueeze(2)  # [1, N, 1, H, W]
fused = (weight_maps_expanded * completions).sum(dim=1)  # [1, 3, H, W]

X_input = X_partial.clone()
mask_expanded = M.expand_as(X_partial)
X_input = X_input * (1 - mask_expanded) + fused * mask_expanded
```

모델이 생성한 가중치 맵은 각 completion 이미지에 적용되고, 그 결과가 합산되어 융합된 이미지(`fused`)를 생성합니다. 이 융합된 이미지는 마스크 영역에만 적용되어 최종 결과를 생성합니다.

## 종합 분석

이 모델은 전형적인 인코더-디코더 아키텍처의 중간에 Transformer를 삽입한 하이브리드 구조입니다. 특징:

1. **초기 컨볼루션**: 이미지 다운샘플링 및 초기 특징 추출
2. **Transformer**: 전역 문맥 포착 및 객체 간 관계 모델링
3. **업샘플링 컨볼루션**: 원래 해상도로 복원 및 픽셀별 가중치 생성

가변적인 completion 수는 고정된 최대값(n_max_completions)과 빈 completion 패딩을 통해 처리됩니다. 모델은 훈련 중에 패딩된 빈 completion에 낮은 가중치를 할당하도록 학습하여 실제 객체가 있는 completion만 선택합니다.

L1 및 LPIPS 손실을 사용하여, 모델은 마스크 영역에서 원본 이미지와 가장 유사한 결과를 생성하도록 훈련됩니다. 이는 자연스럽게 올바른 중첩 순서를 학습하도록 유도합니다.

이런 설계 덕분에, 이 모델은 중첩된 객체들의 복잡한 관계를 이해하고 여러 amodal completion 이미지를 적절하게 융합하여 원본에 가까운 이미지를 생성할 수 있습니다.